1.If 7TB is the available disk space per node (9 disks with 1 TB, 2 disk for operating system etc.
were excluded.). Assuming initial data size is 600 TB. How will you estimate the number of data
nodes (n)?



	The formula to calculate the number of data nodes is:
 
	n= H/d = c*r*S/(1-i)*d 

	H=c*r*s   and d=(1-i)*d

	where

	H is Hadoop storage 

	c is average compression ratio.
   	 Compression ratio refers to type of compression used.If not  take c=1;

	r is Replication factor
    	 Replication factor is usually 3 in production cluster.

	s is size of the data which is needed to be moved to hadoop
         	 It can be a combination of historical data and incremental data.
         	 The incremental data can be projected over a period of time (Take for example 3years) 

	i is Intermediate factor
     	 Intermediate factor is usually  1/3 or 1/4

	d is disk space availability per node


	The solution is:
  	H=600TB   d=7TB

	n=H/d
  	=600/7
  	=85.7
 
	therfore, 85 DATA NODES are needed.

2.Imagine that you are uploading a file of 500MB into HDFS.100MB of data is successfully
uploaded into HDFS and another client wants to read the uploaded data while the upload is still in
progress. What will happen in such a scenario, will the 100 MB of data that is uploaded will it be
displayed?


 * The default size of a block in Hadoop 1x is 64 MB and 128 MB in 2x .
 * Considering the block size here to be 100 MB, we will have 5 blocks replicated three times . 
 * We have 5 blocks for a file , client , name node and data node .
 * First the client wil take the first block and will go to a name node for data node location in order to store this block . 
 * When the client is aware of the datanode information, it will reach the data node and begin to copy the first block . 
 *Now , the client will get confirmation on first block and it will initiate the same process for the second block . 
 * So, The reader will be able to read the 100 MB uploaded data while the remaining upload is in progress . 